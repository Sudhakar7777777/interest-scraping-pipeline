services:
  scraper-app:
    build: ./scraper-service
    container_name: scraper-app
    ports:
      - "8000:8000"
    # environment:
      # - PLAYWRIGHT_BROWSERS_PATH=/usr/local/share/.playwright  # Playwright browser executable path
    # volumes:
    #   - .:/app
    networks:
      - airflow-network
    # restart: always    --- Needed for production like scenario

  # Airflow Web Server
  airflow-webserver:
    build: airflow
    container_name: airflow-webserver
    command: ["airflow", "webserver"]
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+mysqlconnector://airflow:airflow@airflow-db:3306/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=sHCiCmJcwqf1hC6PBuSEu9zInwsLVytixiK1a6_9-Dw=
      - AIRFLOW__WEBSERVER__SECRET_KEY=my-secret-key
    ports:
      - "8080:8080"
    # volumes:
      # - ./airflow/dags:/opt/airflow/dags
      # - ./scraper:/opt/airflow/scraper  # Mount scraper module here
      # - ./airflow/requirements.txt:/tmp/requirements.txt
    networks:
      - airflow-network
    depends_on:
      - airflow-db
      - scraper-app

  # Airflow Scheduler
  airflow-scheduler:
    build: airflow
    container_name: airflow-scheduler
    command: ["airflow", "scheduler"]
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+mysqlconnector://airflow:airflow@airflow-db:3306/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=sHCiCmJcwqf1hC6PBuSEu9zInwsLVytixiK1a6_9-Dw=
    # volumes:
      # - ./airflow/dags:/opt/airflow/dags
      # - ./scraper:/opt/airflow/scraper  # Mount scraper module here
      # - ./airflow/requirements.txt:/tmp/requirements.txt
    depends_on:
      - airflow-webserver
      - airflow-db
    networks:
      - airflow-network

  # MySQL Database (Airflow metadata database)
  airflow-db:
    image: mysql:8.0
    container_name: airflow-db
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_USER: airflow
      MYSQL_PASSWORD: airflow
      MYSQL_DATABASE: airflow
    volumes:
      - airflow-db-data:/var/lib/mysql
    networks:
      - airflow-network

  # # Scraper service
  # scraper:
  #   build: ./scraper
  #   container_name: scraper
  #   environment:
  #     - AIRFLOW_URL=http://airflow-webserver:8080
  #   volumes:
  #     - ./db:/app/db
  #   depends_on:
  #     - airflow-webserver
  #   networks:
  #     - airflow-network

  

  # ELK Stack for Logging (optional)
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
    ports:
      - "9200:9200"
    depends_on:
      - airflow-webserver
    networks:
      - airflow-network

  logstash:
    image: docker.elastic.co/logstash/logstash:7.10.0
    container_name: logstash
    environment:
      - LOGSTASH_INPUT_HTTP_HOST=0.0.0.0
    ports:
      - "5044:5044"
    depends_on:
      - airflow-webserver
    networks:
      - airflow-network

  kibana:
    image: docker.elastic.co/kibana/kibana:7.10.0
    container_name: kibana
    ports:
      - "5601:5601"
    depends_on:
      - airflow-webserver
    networks:
      - airflow-network

  # Prometheus for monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    depends_on:
      - airflow-webserver
    networks:
      - airflow-network

networks:
  airflow-network:
    driver: bridge

volumes:
  airflow-db-data:
